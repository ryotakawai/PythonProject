{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4 データ拡張まで"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage as ndimage\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\210450279\\Anaconda3\\envs\\rebases\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 再現性の確保のために乱数シードを固定（数値は何でもよい）\n",
    "tf.set_random_seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# 入力データ\n",
    "# MNISTのワンホット表現での読み込み\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True, validation_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 入力画像\n",
    "x = tf.placeholder(tf.float32, name='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 サイズ変更\n",
    "x_1 = tf.reshape(x, [-1, 28, 28, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 畳み込み\n",
    "# ランダムカーネル\n",
    "k_0 = tf.Variable(tf.truncated_normal([4, 4, 1, 10], mean=0.0, stddev=0.1))\n",
    "# 畳み込み\n",
    "x_2 = tf.nn.conv2d(x_1, k_0, strides=[1, 3, 3, 1], padding='VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 活性化関数\n",
    "x_3 = tf.nn.relu(x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 プーリング\n",
    "x_4 = tf.nn.max_pool(x_3, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='VALID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 サイズ変更\n",
    "x_5 = tf.reshape(x_4, [-1, 160])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ドロップアウト付きの全結合\n",
    "def matmul_plus_bias_with_dropout(x, w, b, p):\n",
    "    return tf.matmul(tf.nn.dropout(x, keep_prob=p), w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 全結合\n",
    "# 重みとバイアス\n",
    "w_1 = tf.Variable(tf.zeros([160, 40]))\n",
    "b_1 = tf.Variable([0.1] * 40)\n",
    "#ドロップアウト率\n",
    "p_1 = tf.placeholder(tf.float32, name='p_1')\n",
    "# 全結合\n",
    "x_6 = matmul_plus_bias_with_dropout(x_5, w_1, b_1, p_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 活性化関数\n",
    "x_7 = tf.nn.relu(x_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 全結合\n",
    "# 重みとバイアス\n",
    "w_2 = tf.Variable(tf.zeros([40, 10]))\n",
    "b_2 = tf.Variable([0.1] * 10)\n",
    "#ドロップアウト率\n",
    "p_2 = tf.placeholder(tf.float32, name='p_2')\n",
    "# 全結合\n",
    "x_8 = matmul_plus_bias_with_dropout(x_7, w_2, b_2, p_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9 確率化\n",
    "y = tf.nn.softmax(x_8, name='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 損失関数の最小化\n",
    "# 正解ラベル\n",
    "labels = tf.placeholder(tf.float32, name='labels')\n",
    "# 損失関数（交差エントロピー）と最適化処理（Adam）\n",
    "loss = -tf.reduce_sum(labels * tf.log(y + 1e-9)) / tf.cast(tf.shape(y)[0], tf.float32)\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11 精度検証 (Top-1 accuracy)\n",
    "prediction_match = tf.equal(tf.argmax(y, axis=1), tf.argmax(labels, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction_match, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数と精度の出力をトレース対象とする\n",
    "loss_summary = tf.summary.scalar('loss', loss)\n",
    "accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "summary = tf.summary.merge([loss_summary, accuracy_summary])\n",
    "\n",
    "image_summary_op = tf.summary.image('input', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回転\n",
    "def random_rotate_image(image):\n",
    "    image = ndimage.rotate(image, np.random.uniform(-5, 5), reshape=False)\n",
    "    return image\n",
    "\n",
    "# 平行移動\n",
    "def random_shift_image(image):\n",
    "    shift = [*np.random.randint(-3, 3, 2), 0]\n",
    "    image = ndimage.interpolation.shift(image, shift)\n",
    "    return image\n",
    "\n",
    "# # データ拡張\n",
    "def augment(images):\n",
    "    # 画像の変形を行うには、事前に画像の形状変換が必要である (784,) → (28,28,1)\n",
    "    # バリデーションセットやテストセットには、データ拡張は行わない。\n",
    "    images = images.reshape((-1, 28, 28, 1))\n",
    "    preprocessed_images = []  # コピー画像データセットを入れるための空リスト\n",
    "    for image in images:\n",
    "        # データセットの画像自体を書き換えないよう、画像をコピーしてからデータ拡張を適用する\n",
    "        preprocessed_image = image.copy()\n",
    "        preprocessed_image = random_rotate_image(preprocessed_image)\n",
    "        preprocessed_image = random_shift_image(preprocessed_image)\n",
    "        preprocessed_images.append(preprocessed_image)\n",
    "    return preprocessed_images\n",
    "\n",
    "# MinMaxScaler: 正規化（最大1, 最小0）\n",
    "def preprocess(images):\n",
    "    scaler = MinMaxScaler()\n",
    "    return scaler.partial_fit(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメーター\n",
    "# バッチサイズ\n",
    "BATCH_SIZE = 32\n",
    "# 学習回数\n",
    "NUM_TRAIN = 10000\n",
    "# 学習中の出力頻度\n",
    "OUTPUT_BY = 500\n",
    "#ドロップアウト率\n",
    "DROPOUT_PROB_1 = 0.2\n",
    "DROPOUT_PROB_2 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以前のログが残っていれば削除する（通常は残すようにしてください）\n",
    "logdir = Path('logs/log2')\n",
    "if logdir.exists():\n",
    "    shutil.rmtree(str(logdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, train_loss 2.30, val_loss 2.30, train_acc 0.12, val_acc 0.10\n",
      "step 500, train_loss 1.86, val_loss 1.70, train_acc 0.38, val_acc 0.52\n",
      "step 1000, train_loss 1.74, val_loss 1.56, train_acc 0.53, val_acc 0.59\n",
      "step 1500, train_loss 1.49, val_loss 1.41, train_acc 0.62, val_acc 0.63\n",
      "step 2000, train_loss 1.47, val_loss 1.28, train_acc 0.62, val_acc 0.67\n",
      "step 2500, train_loss 1.34, val_loss 1.20, train_acc 0.62, val_acc 0.74\n",
      "step 3000, train_loss 1.33, val_loss 1.15, train_acc 0.59, val_acc 0.72\n",
      "step 3500, train_loss 1.24, val_loss 1.08, train_acc 0.62, val_acc 0.76\n",
      "step 4000, train_loss 1.03, val_loss 1.06, train_acc 0.88, val_acc 0.78\n",
      "step 4500, train_loss 1.30, val_loss 1.02, train_acc 0.62, val_acc 0.78\n",
      "step 5000, train_loss 1.00, val_loss 1.01, train_acc 0.81, val_acc 0.78\n",
      "step 5500, train_loss 1.27, val_loss 0.98, train_acc 0.56, val_acc 0.79\n",
      "step 6000, train_loss 1.19, val_loss 0.95, train_acc 0.62, val_acc 0.80\n",
      "step 6500, train_loss 0.96, val_loss 0.94, train_acc 0.84, val_acc 0.81\n",
      "step 7000, train_loss 1.03, val_loss 0.94, train_acc 0.78, val_acc 0.81\n",
      "step 7500, train_loss 1.13, val_loss 0.92, train_acc 0.72, val_acc 0.80\n",
      "step 8000, train_loss 1.15, val_loss 0.94, train_acc 0.69, val_acc 0.80\n",
      "step 8500, train_loss 1.03, val_loss 0.93, train_acc 0.66, val_acc 0.81\n",
      "step 9000, train_loss 0.91, val_loss 0.91, train_acc 0.78, val_acc 0.81\n",
      "step 9500, train_loss 0.87, val_loss 0.91, train_acc 0.84, val_acc 0.82\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'model2/my-model'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 学習の実行\n",
    "# TensorBoard上でグラフを重ねて描くために、2つのwriterを使用する\n",
    "with tf.summary.FileWriter('logs/log2/train') as train_writer, \\\n",
    "    tf.summary.FileWriter('logs/log2/val') as val_writer:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    dropout_prob = {p_1: DROPOUT_PROB_1, p_2: DROPOUT_PROB_2}\n",
    "    saver = tf.train.Saver()\n",
    "    for i in range(NUM_TRAIN):\n",
    "        batch = mnist.train.next_batch(BATCH_SIZE)\n",
    "        inout = {x: augment(batch[0]), labels: batch[1]} # augment()でデータ拡張を行っている\n",
    "        if i % OUTPUT_BY == 0:\n",
    "            train_loss, train_accuracy, train_summary, image_summary = \\\n",
    "                sess.run([loss, accuracy, summary, image_summary_op], feed_dict={**inout, p_1: 1.0, p_2: 1.0})\n",
    "            val_loss, val_accuracy, val_summary = \\\n",
    "                sess.run([loss, accuracy, summary], feed_dict={\n",
    "                x: mnist.validation.images, labels: mnist.validation.labels, p_1: 1.0, p_2: 1.0})\n",
    "            print('step {:d}, train_loss {:.2f}, val_loss {:.2f}, train_acc {:.2f}, val_acc {:.2f}'.format(\n",
    "                i, train_loss, val_loss, train_accuracy, val_accuracy))\n",
    "            # ログの出力\n",
    "            train_writer.add_summary(train_summary, global_step=i)\n",
    "            val_writer.add_summary(val_summary, global_step=i)\n",
    "            train_writer.add_summary(image_summary, global_step=i)\n",
    "            # 過程の保存\n",
    "            saver.save(sess, 'model2/my-model', global_step=i)\n",
    "            # ファイルへの強制書き出し\n",
    "            train_writer.flush()\n",
    "            val_writer.flush()\n",
    "        optimizer.run(feed_dict={**inout, **dropout_prob})\n",
    "\n",
    "# 最終結果の保存\n",
    "saver.save(sess, 'model2/my-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracy0.82\n"
     ]
    }
   ],
   "source": [
    "# テストデータによる精度検証\n",
    "test_accuracy = accuracy.eval(feed_dict={x : mnist.test.images,\\\n",
    "    labels : mnist.test.labels, p_1 : 1.0, p_2 :1.0})\n",
    "print('test_accuracy{:.2f}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.82\n"
     ]
    }
   ],
   "source": [
    "# 上記はテンソルを用いて正解率を計算しているが、次のようにNumpy形式で正解率を計算することもできる。\n",
    "\n",
    "# test_y は予測結果。\n",
    "test_y = y.eval(feed_dict={x: mnist.test.images, p_1: 1.0, p_2: 1.0}).argmax(axis=1)\n",
    "\n",
    "# test_labels は正解ラベル。\n",
    "test_labels = mnist.test.labels.argmax(axis=1)\n",
    "\n",
    "# 予測結果と正解ラベルから、正解率を計算する。\n",
    "test_accuracy = np.sum(test_y == test_labels) / float(test_y.shape[0])\n",
    "print('test accuracy {:.2f}'.format(test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a1c8c699d14e3903dc5150bb4eeb93a885371f7e5db0b28d0ec5cc4eff72bd11"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('introtensorflow': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
